{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2959c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.7299 - loss: 3.0697\n",
      "Epoch 2/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 185ms/step - accuracy: 0.8311 - loss: 1.1171\n",
      "Epoch 3/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 185ms/step - accuracy: 0.8365 - loss: 1.0475\n",
      "Epoch 4/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 176ms/step - accuracy: 0.8376 - loss: 1.0016\n",
      "Epoch 5/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 174ms/step - accuracy: 0.8395 - loss: 0.9636\n",
      "Epoch 6/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 176ms/step - accuracy: 0.8419 - loss: 0.9230\n",
      "Epoch 7/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 160ms/step - accuracy: 0.8452 - loss: 0.8799\n",
      "Epoch 8/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 159ms/step - accuracy: 0.8481 - loss: 0.8388\n",
      "Epoch 9/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 152ms/step - accuracy: 0.8514 - loss: 0.8061\n",
      "Epoch 10/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 156ms/step - accuracy: 0.8547 - loss: 0.7661\n",
      "Input: hi, how are you?\n",
      "Response: i was a lot of one\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "\n",
    "def custom_standardization(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, r\"[^a-z0-9\\s\\[\\]]\", \"\")\n",
    "    return text\n",
    "def load_and_prepare_data(input_file, label_file, max_samples=5000, max_len=40):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        input_texts = f.readlines()\n",
    "    with open(label_file, 'r', encoding='utf-8') as f:\n",
    "        label_texts = f.readlines()\n",
    "\n",
    "    input_texts = [f\"[sos] {line.strip()} [eos]\" for line in input_texts[:max_samples]]\n",
    "    label_texts = [f\"[sos] {line.strip()} [eos]\" for line in label_texts[:max_samples]]\n",
    "    return input_texts, label_texts\n",
    "\n",
    "def create_text_vectorizer(texts, max_tokens=10000, max_len=40):\n",
    "    vectorizer = TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_sequence_length=max_len,\n",
    "        standardize=custom_standardization\n",
    "    )\n",
    "    vectorizer.adapt(texts)\n",
    "    return vectorizer\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, max_len, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            tf.range(max_len, dtype=tf.float32)[:, tf.newaxis],\n",
    "            tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model\n",
    "        )\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "class MultiHeadAttentionLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "        self.dense = Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0,2,1,3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights\n",
    "\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff_dim, activation='relu'), Dense(d_model)])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha1 = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff_dim, activation='relu'), Dense(d_model)])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        attn1, _ = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "\n",
    "        attn2, _ = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "        return out3\n",
    "# TRANSFORMER MODEL\n",
    "class Transformer(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim, input_vocab_size, target_vocab_size, max_len, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.enc_emb = Embedding(input_vocab_size, d_model)\n",
    "        self.dec_emb = Embedding(target_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(max_len, d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, ff_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, ff_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        self.final_dense = Dense(target_vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_inp, dec_inp = inputs\n",
    "        enc_padding_mask = create_padding_mask(enc_inp)\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(dec_inp)[1])\n",
    "        dec_padding_mask = create_padding_mask(enc_inp)\n",
    "\n",
    "        enc_x = self.enc_emb(enc_inp)\n",
    "        enc_x = self.pos_enc(enc_x)\n",
    "        for layer in self.enc_layers:\n",
    "            enc_x = layer(enc_x, mask=enc_padding_mask, training=training)\n",
    "\n",
    "        dec_x = self.dec_emb(dec_inp)\n",
    "        dec_x = self.pos_enc(dec_x)\n",
    "        for layer in self.dec_layers:\n",
    "            dec_x = layer(dec_x, enc_output=enc_x,\n",
    "                           look_ahead_mask=look_ahead_mask,\n",
    "                           padding_mask=dec_padding_mask,\n",
    "                           training=training)\n",
    "\n",
    "        return self.final_dense(dec_x)\n",
    "\n",
    "# HÀM DỰ ĐOÁN\n",
    "def decode_sequence(model, input_vectorizer, label_vectorizer, text, max_len=40):\n",
    "    vocab = label_vectorizer.get_vocabulary()\n",
    "    start_token = vocab.index('[sos]')\n",
    "    end_token = vocab.index('[eos]')\n",
    "    input_seq = input_vectorizer([f\"[sos] {text.strip()} [eos]\"])\n",
    "    input_seq = tf.cast(input_seq, tf.int32)\n",
    "    output_seq = tf.expand_dims([start_token], 0)\n",
    "    for _ in range(max_len):\n",
    "        predictions = model([input_seq, output_seq], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "        if predicted_id.numpy()[0][0] == end_token:\n",
    "            break\n",
    "        predicted_id = tf.cast(predicted_id, output_seq.dtype)\n",
    "        output_seq = tf.concat([output_seq, predicted_id], axis=-1)\n",
    "    predicted_sentence = [vocab[id] for id in output_seq.numpy()[0] if id < len(vocab)]\n",
    "    return ' '.join(predicted_sentence).replace('[sos]', '').replace('[eos]', '').strip()\n",
    "\n",
    "# CHẠY TRAINING\n",
    "input_file = 'input_texts.txt'\n",
    "label_file = 'label_texts.txt'\n",
    "max_samples = 5000\n",
    "max_len = 40\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "num_layers = 2\n",
    "dropout_rate = 0.1\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "input_texts, label_texts = load_and_prepare_data(input_file, label_file, max_samples, max_len)\n",
    "input_vectorizer = create_text_vectorizer(input_texts, max_tokens=10000, max_len=max_len)\n",
    "label_vectorizer = create_text_vectorizer(label_texts, max_tokens=10000, max_len=max_len)\n",
    "input_data = tf.cast(input_vectorizer(input_texts), tf.int32)\n",
    "label_data = tf.cast(label_vectorizer(label_texts), tf.int32)\n",
    "label_input_data = label_data[:, :-1]\n",
    "label_target_data = label_data[:, 1:]\n",
    "input_vocab_size = len(input_vectorizer.get_vocabulary())\n",
    "target_vocab_size = len(label_vectorizer.get_vocabulary())\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    max_len=max_len,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer.fit([input_data, label_input_data], label_target_data, batch_size=batch_size, epochs=epochs)\n",
    "# TEST\n",
    "test_text = \"hi, how are you?\"\n",
    "response = decode_sequence(transformer, input_vectorizer, label_vectorizer, test_text)\n",
    "print(\"Input:\", test_text)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae74d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Hàm mất mát\n",
    "def loss_function(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: (batch_size, seq_len) tensor int32\n",
    "    y_pred: (batch_size, seq_len, vocab_size) tensor float32 (softmax output)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
    "    loss_ = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    optimizer='adam',\n",
    "    loss=loss_function,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
