{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2959c44",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input_texts.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 287\u001b[0m\n\u001b[0;32m    284\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Load dữ liệu\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m input_texts, label_texts \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Tạo vectorizers\u001b[39;00m\n\u001b[0;32m    290\u001b[0m input_vectorizer \u001b[38;5;241m=\u001b[39m create_text_vectorizer(input_texts, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, max_len\u001b[38;5;241m=\u001b[39mmax_length)\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mload_and_prepare_data\u001b[1;34m(input_file, label_file, max_samples)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_prepare_data\u001b[39m(input_file, label_file, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25000\u001b[39m):\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     32\u001b[0m         input_texts \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(label_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input_texts.txt'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Transformer encoder-decoder (sửa lỗi, chú thích tiếng Việt).\n",
    "Dùng với file input_texts.txt và label_texts.txt (mỗi dòng 1 câu).\n",
    "Chạy được trên môi trường local (Colab/Local Jupyter).\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Dense, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Hàm tiền xử lý văn bản (chuẩn hoá)\n",
    "# ---------------------------\n",
    "def custom_standardization(text):\n",
    "    # chuyển về chữ thường và giữ chữ/số/khoảng trắng và các token [sos]/[eos]\n",
    "    text = tf.strings.lower(text)\n",
    "    # loại bỏ ký tự lạ (giữ a-z, 0-9, khoảng trắng và [] để giữ [sos] [eos])\n",
    "    text = tf.strings.regex_replace(text, r'[^a-z0-9\\s\\[\\]]', '')\n",
    "    # gộp khoảng trắng nhiều thành 1\n",
    "    text = tf.strings.regex_replace(text, r'\\s+', ' ')\n",
    "    return text\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Đọc dữ liệu từ file (local)\n",
    "# ---------------------------\n",
    "def load_and_prepare_data(input_file, label_file, max_samples=25000):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        input_texts = f.read().splitlines()\n",
    "    with open(label_file, 'r', encoding='utf-8') as f:\n",
    "        label_texts = f.read().splitlines()\n",
    "\n",
    "    # loại bỏ các dòng rỗng\n",
    "    input_texts = [t.strip() for t in input_texts if t.strip()]\n",
    "    label_texts = [t.strip() for t in label_texts if t.strip()]\n",
    "\n",
    "    # đồng bộ số mẫu (nếu 2 file có độ dài khác nhau)\n",
    "    n = min(len(input_texts), len(label_texts), max_samples)\n",
    "    input_texts = input_texts[:n]\n",
    "    label_texts = label_texts[:n]\n",
    "\n",
    "    # thêm token bắt đầu/kết thúc\n",
    "    input_texts = [f\"[sos] {t} [eos]\" for t in input_texts]\n",
    "    label_texts = [f\"[sos] {t} [eos]\" for t in label_texts]\n",
    "\n",
    "    print(f\"Đã nạp {n} mẫu.\")\n",
    "    return input_texts, label_texts\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Tạo TextVectorization (vectorizer)\n",
    "# ---------------------------\n",
    "def create_text_vectorizer(texts, max_tokens=10000, max_len=40):\n",
    "    vectorizer = TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_sequence_length=max_len,\n",
    "        standardize=custom_standardization\n",
    "    )\n",
    "    vectorizer.adapt(texts)\n",
    "    return vectorizer\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Positional Encoding\n",
    "# ---------------------------\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]  # (1, max_len, d_model)\n",
    "        self.pos_encoding = tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        return x + self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Multi-Head Attention Layer\n",
    "# ---------------------------\n",
    "class MultiHeadAttentionLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "        self.dense = Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # (batch, heads, seq_len, depth)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            scaled_logits += (mask * -1e9)\n",
    "        attention_weights = tf.nn.softmax(scaled_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (tf.shape(q)[0], -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights\n",
    "\n",
    "# ---------------------------\n",
    "# 6) EncoderLayer & DecoderLayer\n",
    "# ---------------------------\n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff_dim, activation='relu'), Dense(d_model)])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, mask, training=False):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha1 = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff_dim, activation='relu'), Dense(d_model)])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask, training=False):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Mask helper (sinh mask với shape broadcast được)\n",
    "# ---------------------------\n",
    "def create_padding_mask(seq):\n",
    "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]  # (batch,1,1,seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return tf.cast(mask, tf.float32)[tf.newaxis, tf.newaxis, :, :]  # (1,1,size,size)\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Transformer (encoder-decoder)\n",
    "# ---------------------------\n",
    "class Transformer(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim,\n",
    "                 input_vocab_size, target_vocab_size, max_len, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.encoder_embedding = Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(max_len, d_model)\n",
    "\n",
    "        self.encoder_layers = [EncoderLayer(d_model, num_heads, ff_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        self.decoder_layers = [DecoderLayer(d_model, num_heads, ff_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.final_layer = Dense(target_vocab_size)  # logits\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, target_seq = inputs\n",
    "        enc_padding_mask = create_padding_mask(input_seq)\n",
    "        dec_padding_mask = create_padding_mask(input_seq)\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(target_seq)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(target_seq)\n",
    "\n",
    "        # Encoder\n",
    "        enc_emb = self.encoder_embedding(input_seq) * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        enc_emb = self.pos_encoding(enc_emb)\n",
    "        enc_emb = self.dropout(enc_emb, training=training)\n",
    "        enc_output = enc_emb\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, enc_padding_mask, training=training)\n",
    "\n",
    "        # Decoder\n",
    "        dec_emb = self.decoder_embedding(target_seq) * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        dec_emb = self.pos_encoding(dec_emb)\n",
    "        dec_emb = self.dropout(dec_emb, training=training)\n",
    "\n",
    "        dec_output = dec_emb\n",
    "        # Tạo mask self-attention cho decoder: kết hợp look-ahead và padding target\n",
    "        # dec_target_padding_mask: (batch,1,1,dec_seq) -> tile theo chiều seq để thành (batch,1,dec_seq,dec_seq)\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            batch = tf.shape(target_seq)[0]\n",
    "            dec_seq_len = tf.shape(target_seq)[1]\n",
    "            # tile padding mask\n",
    "            dec_pad_tiled = tf.tile(dec_target_padding_mask, [1, 1, dec_seq_len, 1])  # (batch,1,dec_seq,dec_seq)\n",
    "            combined_look_ahead_mask = tf.maximum(look_ahead_mask, dec_pad_tiled)      # (batch,1,dec_seq,dec_seq)\n",
    "            dec_output, _, _ = dec_layer(dec_output, enc_output, combined_look_ahead_mask, dec_padding_mask, training=training)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # logits\n",
    "        return final_output\n",
    "\n",
    "# ---------------------------\n",
    "# 9) Hàm decode (inference) dùng vectorizer\n",
    "# ---------------------------\n",
    "def decode_sequence(model, input_vectorizer, label_vectorizer, input_text, max_len=40):\n",
    "    vocab = label_vectorizer.get_vocabulary()\n",
    "    try:\n",
    "        start_token = vocab.index('[sos]')\n",
    "        end_token = vocab.index('[eos]')\n",
    "    except ValueError:\n",
    "        start_token = 1\n",
    "        end_token = 2\n",
    "\n",
    "    input_seq = input_vectorizer([f\"[sos] {input_text.strip()} [eos]\"])\n",
    "    input_seq = tf.cast(input_seq, tf.int32)\n",
    "\n",
    "    output_seq = tf.expand_dims([start_token], 0)  # (1,1)\n",
    "    for _ in range(max_len):\n",
    "        predictions = model(inputs=[input_seq, output_seq], training=False)  # (1, seq_len, vocab)\n",
    "        predictions = predictions[:, -1:, :]  # (1,1,vocab)\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "        pid = int(predicted_id.numpy()[0][0])\n",
    "        if pid == end_token:\n",
    "            break\n",
    "        output_seq = tf.concat([output_seq, tf.cast(predicted_id, dtype=output_seq.dtype)], axis=-1)\n",
    "\n",
    "    tokens = [vocab[i] if i < len(vocab) else '' for i in output_seq.numpy()[0]]\n",
    "    text = ' '.join(tokens).replace('[sos]', '').replace('[eos]', '').strip()\n",
    "    return text\n",
    "\n",
    "# ---------------------------\n",
    "# 10) Pipeline chính: nạp dữ liệu, vectorize, train\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Đường dẫn file (local)\n",
    "    input_file = \"input_texts.txt\"\n",
    "    label_file = \"label_texts.txt\"\n",
    "\n",
    "    # Siêu tham số (giảm để chạy nhanh)\n",
    "    max_samples = 5000\n",
    "    max_length = 40\n",
    "    d_model = 128\n",
    "    num_heads = 4\n",
    "    ff_dim = 512\n",
    "    num_layers = 2\n",
    "    dropout_rate = 0.1\n",
    "    epochs = 4     # test nhanh: 4 epoch\n",
    "    batch_size = 64\n",
    "\n",
    "    # Load dữ liệu\n",
    "    input_texts, label_texts = load_and_prepare_data(input_file, label_file, max_samples)\n",
    "\n",
    "    # Tạo vectorizers\n",
    "    input_vectorizer = create_text_vectorizer(input_texts, max_tokens=10000, max_len=max_length)\n",
    "    label_vectorizer = create_text_vectorizer(label_texts, max_tokens=10000, max_len=max_length)\n",
    "\n",
    "    # Vectorize\n",
    "    input_data = input_vectorizer(input_texts)\n",
    "    input_data = tf.cast(input_data, tf.int32)\n",
    "    label_data = label_vectorizer(label_texts)\n",
    "    label_data = tf.cast(label_data, tf.int32)\n",
    "\n",
    "    # Chuẩn bị decoder input và target (shift right)\n",
    "    label_input_data = label_data[:, :-1]\n",
    "    label_target_data = label_data[:, 1:]\n",
    "\n",
    "    # Kích thước từ vựng\n",
    "    input_vocab_size = len(input_vectorizer.get_vocabulary())\n",
    "    target_vocab_size = len(label_vectorizer.get_vocabulary())\n",
    "    print(f\"Số mẫu: {input_data.shape[0]} | Vocab input: {input_vocab_size} | Vocab target: {target_vocab_size}\")\n",
    "\n",
    "    # Tạo dataset TF\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((input_data, label_input_data), label_target_data))\n",
    "    train_dataset = train_dataset.shuffle(1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Build model\n",
    "    transformer = Transformer(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        ff_dim=ff_dim,\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        target_vocab_size=target_vocab_size,\n",
    "        max_len=max_length,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "\n",
    "    # Loss và metric: dùng sparse categorical crossentropy từ logits, và mask pad\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "    def masked_loss(y_true, y_pred):\n",
    "        mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    def masked_accuracy(y_true, y_pred):\n",
    "        y_pred_ids = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "        matches = tf.cast(tf.equal(y_true, y_pred_ids), tf.float32)\n",
    "        mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "        matches *= mask\n",
    "        return tf.reduce_sum(matches) / tf.reduce_sum(mask)\n",
    "\n",
    "    transformer.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=masked_loss,\n",
    "        metrics=[masked_accuracy]\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    transformer.fit(train_dataset, epochs=epochs)\n",
    "\n",
    "    # Lưu model nếu muốn:\n",
    "    # transformer.save_weights(\"transformer_weights.h5\")\n",
    "\n",
    "    # Kiểm thử inference vài câu\n",
    "    test_sentences = [\"hello\", \"how are you\", \"what is your name\", \"tell me a joke\"]\n",
    "    for s in test_sentences:\n",
    "        print(\"Input:\", s)\n",
    "        resp = decode_sequence(transformer, input_vectorizer, label_vectorizer, s, max_len=40)\n",
    "        print(\"Response:\", resp)\n",
    "        print(\"-\"*40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
